{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWBEcE34VXVvPzI9zC6fhV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArchanaSahoo89/Statistics_Advance_1_Assignment/blob/main/Statistics_Advance_1_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    1. Explain the properties of the F-distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The F-distribution is a continuous probability distribution that arises frequently in the context of statistical inference, particularly in analysis of variance (ANOVA) and regression analysis. Here are the key properties of the F-distribution:\n",
        "\n",
        "Definition:\n",
        "\n",
        " The F-distribution is defined as the ratio of two scaled chi-squared distributions. Specifically, if ( X ) and ( Y ) are independent chi-squared random variables with ( d_1 ) and ( d_2 ) degrees of freedom, respectively, then the random variable ( F = \\frac{X/d_1}{Y/d_2} ) follows an F-distribution with ( d_1 ) and ( d_2 ) degrees of freedom.\n",
        "\n",
        "Degrees of Freedom:\n",
        "\n",
        " The F-distribution is characterized by two parameters, referred to as degrees of freedom: ( d_1 ) (numerator degrees of freedom) and ( d_2 ) (denominator degrees of freedom). The shape of the distribution depends on these two parameters.\n",
        "\n",
        "Shape:\n",
        "\n",
        " The F-distribution is right-skewed, meaning it has a longer tail on the right side. As the degrees of freedom increase, the distribution becomes more symmetric and approaches a normal distribution.\n",
        "\n",
        "Range:\n",
        "\n",
        "The F-distribution only takes on positive values, ranging from 0 to ( \\infty ).\n",
        "\n",
        "Mean: The mean of the F-distribution is given by: [ \\text{Mean} = \\frac{d_1}{d_1 - 2} \\quad \\text{for } d_1 > 2. ] If ( d_1 \\leq 2 ), the mean is undefined.\n",
        "\n",
        "Variance: The variance of the F-distribution is: [ \\text{Variance} = \\frac{2(d_1^2)(d_2^2)(d_2 + 1)}{d_1^2(d_2 - 2)^2(d_2 - 4)} \\quad \\text{for } d_2 > 4. ] If ( d_2 \\leq 4 ), the variance is undefined.\n",
        "\n",
        "Moment Generating Function (MGF): The moment-generating function of the F-distribution does not exist in a simple closed form, and it is not defined for all values.\n",
        "\n",
        "Applications: The F-distribution is commonly used in hypothesis testing, particularly in ANOVA, to compare variances between groups. It is also used in regression analysis to assess the overall significance of a model.\n",
        "\n",
        "Critical Values: Critical values from the F-distribution can be obtained from F-distribution tables or computed using statistical software. These values are used to determine whether to reject the null hypothesis in hypothesis testing.\n",
        "\n",
        "Relationship to Other Distributions: The F-distribution is related to other distributions, such as the chi-squared distribution and the beta distribution. Specifically, the F-distribution can be derived from the beta distribution and vice versa.\n",
        "\n",
        "Understanding these properties is essential for effectively using the F-distribution in statistical analysis and inference.\n"
      ],
      "metadata": {
        "id": "ZFDIzmTVgWWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "\n",
        "\n",
        "\n",
        "The F-distribution is primarily used in several types of statistical tests, particularly those that involve comparing variances or assessing the significance of multiple groups. Here are the main types of statistical tests that utilize the F-distribution and explanations of why it is appropriate for these tests:\n",
        "\n",
        "Analysis of Variance (ANOVA):\n",
        "\n",
        "Purpose: ANOVA is used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others.\n",
        "Why F-distribution: ANOVA tests the null hypothesis that all group means are equal by comparing the variance between groups to the variance within groups. The ratio of these variances follows an F-distribution under the null hypothesis. If the F-statistic is significantly larger than expected under the null hypothesis, it suggests that at least one group mean is different.\n",
        "\n",
        "\n",
        "Regression Analysis:\n",
        "\n",
        "Purpose: In regression analysis, the F-test is used to assess the overall significance of a regression model, specifically to test whether the independent variables collectively have a significant effect on the dependent variable.\n",
        "\n",
        "\n",
        "Why F-distribution:\n",
        "\n",
        "The F-statistic is calculated as the ratio of the explained variance (due to the regression) to the unexplained variance (residual). This ratio follows an F-distribution when the null hypothesis (that the independent variables do not explain any variance in the dependent variable) is true.\n",
        "Comparing Two Variances:\n",
        "\n",
        "Purpose: The F-test can be used to compare the variances of two populations to determine if they are significantly different.\n",
        "Why F-distribution: The test statistic is calculated as the ratio of the two sample variances. Under the null hypothesis that the two population variances are equal, this ratio follows an F-distribution.\n",
        "\n",
        "MANOVA (Multivariate Analysis of Variance):\n",
        "\n",
        "Purpose: MANOVA extends ANOVA to multiple dependent variables, assessing whether the means of these variables differ across groups.\n",
        "Why F-distribution: Similar to ANOVA, MANOVA uses an F-statistic to compare the variances between groups to the variances within groups across multiple dependent variables.\n",
        "\n",
        "ANCOVA (Analysis of Covariance):\n",
        "\n",
        "Purpose: ANCOVA combines ANOVA and regression to evaluate whether population means of a dependent variable (adjusted for covariates) differ across levels of a categorical independent variable.\n",
        "Why F-distribution: ANCOVA uses the F-statistic to test the significance of the categorical independent variable while controlling for the effects of covariates, relying on the properties of the F-distribution for inference.\n",
        "\n",
        "Why the F-distribution is Appropriate:\n",
        "\n",
        "Ratio of Variances: The F-distribution arises naturally when comparing variances from two or more groups. Since variance is a measure of spread, the F-distribution effectively captures the relationship between the variability between groups and within groups.\n",
        "\n",
        "Assumptions: The F-test assumes that the populations from which the samples are drawn are normally distributed and that the samples are independent. Under these conditions, the F-distribution provides a valid framework for hypothesis testing.\n",
        "\n",
        "Robustness: The F-test is robust to certain violations of normality, particularly when sample sizes are large due to the Central Limit Theorem. This makes it a widely applicable tool in various statistical analyses.\n",
        "\n",
        "In summary, the F-distribution is essential for various statistical tests that involve comparing variances and assessing the significance of group differences, making it a fundamental component of inferential statistics.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4f6XIG-TgzqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    3. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "    populations?\n",
        "\n",
        "\n",
        "\n",
        "When conducting an F-test to compare the variances of two populations, several key assumptions must be met to ensure the validity of the test results. These assumptions are crucial for the F-test to be appropriate and for the conclusions drawn from the test to be reliable. Here are the primary assumptions:\n",
        "\n",
        "Independence:\n",
        "\n",
        "The samples drawn from the two populations must be independent of each other. This means that the selection of one sample does not influence the selection of the other sample. Independence is a critical assumption in most statistical tests, including the F-test.\n",
        "\n",
        "Normality:\n",
        "\n",
        "The populations from which the samples are drawn should be normally distributed. While the F-test is somewhat robust to violations of normality, especially with larger sample sizes (due to the Central Limit Theorem), significant departures from normality can affect the validity of the test results, particularly with small sample sizes.\n",
        "\n",
        "Homogeneity of Variance:\n",
        "\n",
        "The F-test specifically tests the null hypothesis that the two population variances are equal. Therefore, it is assumed that the variances of the two populations are equal (homoscedasticity). If this assumption is violated, the F-test may yield misleading results.\n",
        "\n",
        "Random Sampling:\n",
        "\n",
        "The samples should be drawn randomly from their respective populations. This ensures that the samples are representative of the populations, which is essential for generalizing the results.\n",
        "\n",
        "Continuous Data:\n",
        "\n",
        "The data being analyzed should be continuous. The F-test is designed for continuous variables, and using it with categorical data or ordinal data may not be appropriate.\n",
        "\n",
        "Additional Considerations:\n",
        "\n",
        "Sample Size: While not a formal assumption, larger sample sizes can help mitigate the effects of violations of normality and homogeneity of variance. If sample sizes are small, it's particularly important to check the normality of the data.\n",
        "\n",
        "Outliers:\n",
        "\n",
        "The presence of outliers can significantly affect the results of the F-test. It is advisable to check for outliers and consider their impact on the variances before conducting the test.\n",
        "\n",
        "If these assumptions are not met, alternative methods or tests may be considered, such as the Levene's test or the Brown-Forsythe test, which are designed to test for equality of variances without the assumption of normality.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zSCDzKa6iZbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    4. What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Purpose of ANOVA\n",
        "\n",
        "ANOVA (Analysis of Variance) is a statistical method used to determine whether there are statistically significant differences between the means of three or more independent groups. The primary purposes of ANOVA include:\n",
        "\n",
        "Comparing Multiple Groups:\n",
        "\n",
        "ANOVA allows researchers to compare the means of three or more groups simultaneously, rather than conducting multiple t-tests, which could increase the risk of Type I error (the incorrect rejection of a true null hypothesis).\n",
        "\n",
        "Testing Hypotheses:\n",
        "\n",
        "ANOVA tests the null hypothesis that all group means are equal against the alternative hypothesis that at least one group mean is different. This is particularly useful in experimental designs where one wants to assess the effect of a categorical independent variable on a continuous dependent variable.\n",
        "\n",
        "Assessing Variability:\n",
        "\n",
        "ANOVA evaluates how much of the total variability in the data can be attributed to the differences between group means versus the variability within the groups. This helps to understand the impact of the independent variable on the dependent variable.\n",
        "\n",
        "Extension of t-test:\n",
        "\n",
        "ANOVA can be seen as an extension of the t-test for situations where there are more than two groups.\n",
        "\n",
        "Differences Between ANOVA and t-test\n",
        "\n",
        "While both ANOVA and t-tests are used to compare means, they differ in several key aspects:\n",
        "\n",
        "Number of Groups:\n",
        "\n",
        "t-test: Typically used to compare the means of two groups (independent samples t-test) or the means of one group against a known value (one-sample t-test).\n",
        "\n",
        "ANOVA:\n",
        "\n",
        "Used to compare the means of three or more groups.\n",
        "Hypotheses:\n",
        "\n",
        "t-test:\n",
        "\n",
        "Tests the null hypothesis that the means of two groups are equal (e.g., ( H_0: \\mu_1 = \\mu_2 )).\n",
        "\n",
        "ANOVA:\n",
        "\n",
        "Tests the null hypothesis that all group means are equal (e.g., ( H_0: \\mu_1 = \\mu_2 = \\mu_3 = ... = \\mu_k )).\n",
        "Type I Error Rate:\n",
        "\n",
        "t-test: Conducting multiple t-tests increases the risk of Type I error, as each test has its own significance level (e.g., 0.05). If you perform several t-tests, the cumulative probability of making at least one Type I error increases.\n",
        "\n",
        "ANOVA: By using ANOVA to compare all groups at once, the overall Type I error rate is controlled, maintaining the desired significance level.\n",
        "Output:\n",
        "\n",
        "t-test: Provides a t-statistic and p-value to determine significance.\n",
        "ANOVA: Produces an F-statistic and associated p-value, which indicates whether there are significant differences among the group means.\n",
        "Post Hoc Tests:\n",
        "\n",
        "t-test: If the t-test is significant, you typically just report the result.\n",
        "ANOVA: If the ANOVA indicates significant differences, post hoc tests (such as Tukey's HSD or Bonferroni) are often conducted to determine which specific groups differ from each other.\n",
        "\n",
        "Summary\n",
        "\n",
        "ANOVA is a powerful statistical technique used for comparing means across multiple groups, while t-tests are more suitable for comparing means between two groups. ANOVA helps control for Type I error rates when making multiple comparisons and provides a framework for further analysis if significant differences are found."
      ],
      "metadata": {
        "id": "dIs9mFu6yN2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "     5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "     than two groups.\n",
        "\n",
        "\n",
        "A one-way ANOVA is used instead of multiple t-tests when comparing more than two groups to control the Type I error rate. Each t-test increases the chance of incorrectly rejecting the null hypothesis, so using ANOVA allows for a single test to assess differences among all groups simultaneously. This approach is more efficient and statistically sound.\n",
        "\n",
        "Key Reasons to Use One-Way ANOVA\n",
        "\n",
        "Control of Type I Error Rate:\n",
        "\n",
        "Conducting multiple t-tests increases the likelihood of making a Type I error (incorrectly rejecting the null hypothesis). For example, if you perform three t-tests, the cumulative error rate can exceed the desired 5%, leading to misleading conclusions.\n",
        "\n",
        "One-way ANOVA maintains the Type I error rate at a predetermined level (usually 5%) by evaluating all groups in a single test.\n",
        "\n",
        "Comparison of Multiple Groups:\n",
        "\n",
        "One-way ANOVA is specifically designed to compare three or more groups simultaneously, making it more efficient than performing multiple pairwise t-tests.\n",
        "\n",
        "It assesses whether there are any statistically significant differences among the group means, providing a comprehensive view of the data.\n",
        "\n",
        "Reduction of Complexity:\n",
        "\n",
        "Using one-way ANOVA simplifies the analysis process. Instead of managing multiple tests and their associated results, a single ANOVA test provides a clear outcome regarding group differences.\n",
        "\n",
        "It allows researchers to focus on interpreting one set of results rather than multiple outputs from several t-tests.\n",
        "\n",
        "Assumptions and Conditions:\n",
        "\n",
        "One-way ANOVA assumes that the samples are independent, normally distributed, and have equal variances (homogeneity of variance). If these assumptions are met, ANOVA is a robust method for comparing group means.\n",
        "\n",
        "\n",
        "\n",
        "In summary, one-way ANOVA is preferred over multiple t-tests when comparing more than two groups due to its ability to control the Type I error rate, its efficiency in handling multiple comparisons, and its simplification of the analysis process. This makes it a powerful tool for researchers looking to draw valid conclusions from their data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nangrfL48sLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "    How does this partitioning contribute to the calculation of the F-statistic?\n",
        "\n",
        "\n",
        "\n",
        "In ANOVA (Analysis of Variance), variance is partitioned into two main components: between-group variance and within-group variance. This partitioning is fundamental to understanding how ANOVA tests the hypothesis that group means are equal.\n",
        "\n",
        "1. Between-Group Variance:\n",
        "\n",
        "Definition: Between-group variance measures how much the group means differ from the overall mean of all groups. It reflects the variability that can be attributed to the differences between the groups.\n",
        "\n",
        "Calculation:\n",
        "\n",
        "First, compute the overall mean of all observations (grand mean).\n",
        "For each group, calculate the mean, and then find the squared difference between each group mean and the grand mean.\n",
        "Multiply each squared difference by the number of observations in that group.\n",
        "Sum these values across all groups.\n",
        "[ \\text{Between-group variance} = \\frac{\\sum_{i=1}^{k} n_i (\\bar{X}_i - \\bar{X})^2}{k - 1} ]\n",
        "\n",
        "where:\n",
        "\n",
        "( k ) = number of groups,\n",
        "( n_i ) = number of observations in group ( i ),\n",
        "( \\bar{X}_i ) = mean of group ( i ),\n",
        "( \\bar{X} ) = overall mean.\n",
        "\n",
        "2. Within-Group Variance:\n",
        "\n",
        "Definition: Within-group variance measures how much the individual observations within each group differ from their respective group mean. It reflects the variability that exists within each group.\n",
        "\n",
        "Calculation:\n",
        "\n",
        "For each group, calculate the squared difference between each observation and its group mean.\n",
        "Sum these squared differences across all groups.\n",
        "[ \\text{Within-group variance} = \\frac{\\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_i)^2}{N - k} ]\n",
        "\n",
        "where:\n",
        "\n",
        "( N ) = total number of observations,\n",
        "( X_{ij} ) = ( j )-th observation in group ( i ),\n",
        "( n_i ) = number of observations in group ( i ),\n",
        "( \\bar{X}_i ) = mean of group ( i ).\n",
        "\n",
        "3. F-statistic Calculation:\n",
        "\n",
        "The F-statistic is the ratio of the between-group variance to the within-group variance. It is calculated as follows:\n",
        "\n",
        "[ F = \\frac{\\text{Mean Square Between}}{\\text{Mean Square Within}} = \\frac{\\text{Between-group variance}}{\\text{Within-group variance}} ]\n",
        "\n",
        "Mean Square Between (MSB): This is the between-group variance divided by its degrees of freedom (k - 1):\n",
        "\n",
        "[ \\text{MSB} = \\frac{\\text{Between-group sum of squares}}{k - 1} ]\n",
        "\n",
        "Mean Square Within (MSW): This is the within-group variance divided by its degrees of freedom (N - k):\n",
        "\n",
        "[ \\text{MSW} = \\frac{\\text{Within-group sum of squares}}{N - k} ]\n",
        "\n",
        "4. Interpretation of the F-statistic:\n",
        "\n",
        "The F-statistic indicates the ratio of the variance explained by the group differences (between-group variance) to the variance that is unexplained (within-group variance).\n",
        "\n",
        "A larger F-statistic suggests that the between-group variance is significantly larger than the within-group variance, indicating that at least one group mean is different from the others.\n",
        "If the F-statistic is greater than the critical value from the F-distribution (based on the desired significance level), we reject the null hypothesis that all group means are equal.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "The partitioning of variance into between-group and within-group components is essential for understanding how ANOVA works. It allows researchers to assess whether the variability in the data is primarily due to differences between groups or due to random variation within groups, thus facilitating the calculation and interpretation of the F-statistic."
      ],
      "metadata": {
        "id": "Sw-NI76KVTuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "    differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  The classical (frequentist) approach to ANOVA and the Bayesian approach differ fundamentally in their philosophies, methods of handling uncertainty, parameter estimation, and hypothesis testing. Below are the key differences between the two approaches:\n",
        "\n",
        "1. Handling Uncertainty\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "Uncertainty is handled through the concept of sampling distributions. Parameters (like means and variances) are considered fixed but unknown quantities, and inferences are made based on the likelihood of observing the data given these fixed parameters.\n",
        "Confidence intervals are used to quantify uncertainty. A 95% confidence interval means that if the same experiment were repeated many times, 95% of the calculated intervals would contain the true parameter value.\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "Uncertainty is treated differently; parameters are considered random variables with their own probability distributions (prior distributions). This allows for a direct representation of uncertainty about the parameter values.\n",
        "Bayesian credible intervals are used instead of confidence intervals. A 95% credible interval means there is a 95% probability that the true parameter value lies within that interval, given the observed data and the prior distribution.\n",
        "2. Parameter Estimation\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "Parameters are estimated using point estimates, such as the sample mean. The estimates are derived from the data alone, without incorporating prior beliefs or information.\n",
        "The estimation is typically done using methods like the method of moments or maximum likelihood estimation (MLE).\n",
        "Bayesian Approach:\n",
        "\n",
        "Parameters are estimated using posterior distributions, which combine prior beliefs about the parameters (prior distributions) with the likelihood of the observed data.\n",
        "The posterior distribution is obtained using Bayes' theorem, allowing for the incorporation of prior information and yielding a full distribution of parameter estimates rather than just point estimates.\n",
        "3. Hypothesis Testing\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "Hypothesis testing is done using p-values, which indicate the probability of observing data as extreme as (or more extreme than) the observed data, assuming the null hypothesis is true.\n",
        "The null hypothesis is typically a statement of no effect (e.g., all group means are equal). Decisions are made based on whether the p-value is below a predetermined significance level (e.g., 0.05).\n",
        "Frequentist methods do not provide a direct probability of the hypothesis being true or false.\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "Hypothesis testing is framed in terms of the posterior probabilities of hypotheses. For example, one can calculate the probability that one group mean is greater than another based on the posterior distributions.\n",
        "Bayesian hypothesis testing often involves comparing models (e.g., using Bayes factors) to evaluate the strength of evidence for one hypothesis over another.\n",
        "The Bayesian approach allows for a more intuitive interpretation of results, as it provides probabilities regarding the hypotheses themselves.\n",
        "4. Interpretation of Results\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "Results are often interpreted in the context of long-run frequencies. For instance, a significant result indicates that, under repeated sampling, the null hypothesis would be rejected a certain percentage of the time.\n",
        "The focus is on controlling Type I and Type II errors.\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "Results are interpreted in terms of the probability of parameters and hypotheses given the data. This allows for more nuanced conclusions that incorporate prior knowledge and beliefs.\n",
        "The emphasis is on updating beliefs in light of new evidence.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "In summary, the classical (frequentist) approach to ANOVA relies on fixed parameter estimation, p-values for hypothesis testing, and long-run frequency interpretations of uncertainty. In contrast, the Bayesian approach incorporates prior beliefs into parameter estimation, uses posterior distributions for uncertainty quantification, and provides a probabilistic framework for hypothesis testing. Each approach has its strengths and weaknesses, and the choice between them often depends on the specific context of the analysis, the availability of prior information, and the preferences of the researcher."
      ],
      "metadata": {
        "id": "YAWb1UzPW6zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.Question: You have two sets of data representing the incomes of two different professions\n",
        "#  Profession A: [48, 52, 55, 60, 62]\n",
        "#  Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions' incomes are equal. What are your conclusions based on the F-test?\n",
        "# Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "# Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.\n",
        "\n",
        "\n",
        "# Solution:\n",
        "\n",
        "# To perform an F-test to determine if the variances of the two professions' incomes are equal, we can use Python. The F-test compares the variances of two independent samples. The null hypothesis (H_0) states that the variances are equal, while the alternative hypothesis (H_a) states that the variances are not equal.\n",
        "# Here is the python code to perform F-test on the given data:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for Profession A and Profession B\n",
        "profession_a = np.array([48, 52, 55, 60, 62])\n",
        "profession_b = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "# Calculate variances\n",
        "var_a = np.var(profession_a, ddof=1)  # Sample variance\n",
        "var_b = np.var(profession_b, ddof=1)  # Sample variance\n",
        "\n",
        "# Calculate the F-statistic\n",
        "f_statistic = var_a / var_b\n",
        "\n",
        "# Calculate the degrees of freedom\n",
        "df_a = len(profession_a) - 1\n",
        "df_b = len(profession_b) - 1\n",
        "\n",
        "# Calculate the p-value\n",
        "p_value = 1 - stats.f.cdf(f_statistic, df_a, df_b)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Variance of Profession A: {var_a}\")\n",
        "print(f\"Variance of Profession B: {var_b}\")\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"p-value: {p_value}\")\n",
        "\n",
        "# Conclusion based on p-value\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The variances are not significantly different.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFFs3EBJawTT",
        "outputId": "0276f2d0-fc5a-4630-85de-46d5c36e68b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance of Profession A: 32.8\n",
            "Variance of Profession B: 15.7\n",
            "F-statistic: 2.089171974522293\n",
            "p-value: 0.24652429950266952\n",
            "Fail to reject the null hypothesis: The variances are not significantly different.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data\n",
        "# Region A: [160, 162, 165, 158, 164]\n",
        "# Region B: [172, 175, 170, 168, 174]\n",
        "# Region C: [180, 182, 179, 185, 183]\n",
        "# Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "# Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n",
        "\n",
        "\n",
        "# Solution:\n",
        "\n",
        "\n",
        "# To perform a one-way ANOVA to test whether there are statistically significant differences in average heights between three different regions, we can use the scipy.stats library in Python. The one-way ANOVA will help us determine if there are any significant differences in the means of the three groups.\n",
        "#Here is the python code to perform one-way ANOVA Test:\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for the three regions\n",
        "region_a = np.array([160, 162, 165, 158, 164])\n",
        "region_b = np.array([172, 175, 170, 168, 174])\n",
        "region_c = np.array([180, 182, 179, 185, 183])\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "# Print the results\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"p-value: {p_value}\")\n",
        "\n",
        "# Conclusion based on p-value\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There are statistically significant differences in average heights between the regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There are no statistically significant differences in average heights between the regions.\")\n",
        "\n",
        "\n",
        "# As the p-value is less than 0.05, there is a statistically significant difference between the average heights of people in the three regions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp33ccPKh0NU",
        "outputId": "e5ae5f97-e68a-424b-996b-4682add9c02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n",
            "Reject the null hypothesis: There are statistically significant differences in average heights between the regions.\n"
          ]
        }
      ]
    }
  ]
}